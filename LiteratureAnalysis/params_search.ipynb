{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import nltk\n",
    "from tools import *\n",
    "import GetData\n",
    "from collections import Counter\n",
    "import sklearn.utils as us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_stemmer = nltk.LancasterStemmer()\n",
    "stop_words = GetData.get_stopwords(\"data/stopwords.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, shuffle=True):\n",
    "    rawdata = pd.read_csv(filename, sep=\"|\")\n",
    "    if shuffle:\n",
    "        rawdata = us.shuffle(rawdata,random_state=1994)  # shuffle data\n",
    "    return rawdata\n",
    "\n",
    "def preprocess(data, content):\n",
    "\n",
    "    # word tokenize\n",
    "    X_wt = [word_tokenize(x) for x in GetData.get_content(data, content)]\n",
    "    # 去停用词及文末句号\n",
    "    X_st = [[del_tail_dot(word.lower()) for word in document if not (word.lower() in stop_words or word == \"\")]\n",
    "                  for document in X_wt]\n",
    "    X_ls = [[lancaster_stemmer.stem(word) for word in document] for document in X_st]\n",
    "    # 字符串列表连接\n",
    "    X_ls_merge = [MergeWord(document) for document in X_ls]\n",
    "    return X_ls_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(data, train=True):\n",
    "    \"\"\"\n",
    "    数据标准化\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        ss = StandardScaler()\n",
    "        train_scale = ss.fit_transform(data)\n",
    "        save_model(ss, \"model/flow/ss\")\n",
    "        return train_scale\n",
    "    elif not train:\n",
    "        ss = load_model(\"model/flow/ss\")\n",
    "        data = ss.transform(data)\n",
    "        return data\n",
    "    else:\n",
    "        raise Exception(\"Input correct values of train: True or false\")\n",
    "\n",
    "def get_TfIdf(data,train=True):\n",
    "    \"\"\"\n",
    "    获取TfIdf特征\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        TfidfVec = TfidfVectorizer(max_df=0.1, min_df=0.01, ngram_range=(1,2),stop_words='english')\n",
    "        dataTfIdf = TfidfVec.fit_transform(data)\n",
    "        save_model(TfidfVec, \"model/flow/tfidf\")\n",
    "        return dataTfIdf.toarray()\n",
    "    elif not train:\n",
    "        model = load_model(\"model/flow/tfidf\")\n",
    "        return model.transform(data).toarray()\n",
    "    else:\n",
    "        raise Exception(\"Input correct values of train: True or false\")\n",
    "\n",
    "\n",
    "def get_LDA(data, train=True):\n",
    "    \"\"\"\n",
    "    获取LDA特征\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        CntVec = CountVectorizer(min_df=0.01, ngram_range=(1, 1))\n",
    "        lda = LatentDirichletAllocation(n_components=100,learning_method='batch',\n",
    "                                    random_state=0)\n",
    "        data = CntVec.fit_transform(data)\n",
    "        data = lda.fit_transform(data)\n",
    "        save_model(CntVec, \"model/flow/CntVec\")\n",
    "        save_model(lda, \"model/flow/LDA\")\n",
    "        return data\n",
    "    elif not train:\n",
    "        cntmodel = load_model(\"model/flow/CntVec\")\n",
    "        ldampdel = load_model(\"model/flow/LDA\")\n",
    "        data = cntmodel.transform(data)\n",
    "        data = ldampdel.transform(data)\n",
    "        return data\n",
    "    else:\n",
    "        raise Exception(\"Input correct values of train: True or false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, strategy):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # clf = SVC(C=1,kernel='rbf',probability=True, gamma='scale') # svc without class_weight\n",
    "    # clf = SVC(C=10,kernel='rbf',class_weight='balanced',probability=True, gamma='scale')  # svc with class_weight\n",
    "    clf = XGBClassifier(subsample=0.8, colsample_bytree=0.8, max_depth=5,n_estimators=200)\n",
    "    # clf = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=5,\n",
    "    #                     min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "    #                     objective='binary:logistic', nthread=4, scale_pos_weight=1)\n",
    "    print(clf)\n",
    "    if strategy=='ovr':  # OneVsRest strategy also known as BinaryRelevance strategy\n",
    "        ovr = OneVsRestClassifier(clf)\n",
    "        ovr.fit(X, y)\n",
    "        save_model(ovr, \"model/flow/ovr\")\n",
    "        return ovr\n",
    "    elif strategy=='classifier_chains':\n",
    "        cc = ClassifierChain(clf)\n",
    "        cc.fit(X, y)\n",
    "        save_model(cc, \"model/flow/cc\")\n",
    "        return cc\n",
    "    else:\n",
    "        raise Exception(\"Correct strategies：ovr or classifier_chains\")\n",
    "\n",
    "def predict(data):\n",
    "    pass\n",
    "\n",
    "def evaluation(y_test, preds):\n",
    "    print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess data......\n",
      "statistics of labels:\n",
      "[('001', 229), ('010', 82), ('011', 16), ('100', 33), ('101', 3), ('110', 2), ('111', 1)]\n",
      "generating features......\n",
      "the shape of tfidf features:  1382\n",
      "the shape of LDA features:  100\n",
      "(366, 1482)\n",
      "(41, 1482)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_name = \"data/Data_flow/trainset.csv\"\n",
    "trainset= read_data(train_name)\n",
    "save_model(trainset['DOI'], \"data/Data_flow/trian_DOIs\")\n",
    "print(\"preprocess data......\")\n",
    "X_train_abs = preprocess(trainset,'N_ABS')\n",
    "X_train_titkw = preprocess(trainset,['TITLE', 'KEY_WORDS'])\n",
    "y_train = [get_multiple_label(x, ['F','I','P']) for x in trainset['FLOW']]\n",
    "# print(X_train_titkw[0:10])\n",
    "# print(trainset.TITLE.head(10))\n",
    "y_train = np.array([get_multiple_label(x, ['F','I','P']) for x in trainset['FLOW']])\n",
    "# print(y_train[0:10])\n",
    "\n",
    "test_name = \"data/Data_flow/testset.csv\"\n",
    "testset= read_data(test_name, shuffle=False)\n",
    "X_test_abs = preprocess(testset, 'N_ABS')\n",
    "X_test_titkw = preprocess(testset, ['TITLE','KEY_WORDS'])\n",
    "y_test = np.array([get_multiple_label(x, ['F', 'I', 'P']) for x in testset['FLOW']])\n",
    "# print(testset[0:10])\n",
    "# print(X_test_titkw[0:10])\n",
    "# print(y_test)\n",
    "\n",
    "# 标签计数\n",
    "print(\"statistics of labels:\")\n",
    "target = [''.join(list(map(str, e))) for e in y_train]\n",
    "print(sorted(Counter(target).items()))\n",
    "\n",
    "print(\"generating features......\")\n",
    "X_train_abs = get_TfIdf(X_train_abs, train=True)\n",
    "print(\"the shape of tfidf features: \", X_train_abs.shape[1])\n",
    "X_test_abs = get_TfIdf(X_test_abs, train=False)\n",
    "# print(X_test_abs[0])\n",
    "# print(X_test_abs.shape)\n",
    "\n",
    "X_train_titkw = get_LDA(X_train_titkw, train=True)\n",
    "print(\"the shape of LDA features: \", X_train_titkw.shape[1])\n",
    "X_test_titkw = get_LDA(X_test_titkw, train=False)\n",
    "# print(X_test_titkw[0])\n",
    "# print(X_test_titkw.shape)\n",
    "\n",
    "# merge data\n",
    "# print(X_train_abs.shape, X_train_titkw.shape)\n",
    "# print(X_test_abs.shape, X_test_titkw.shape)\n",
    "X_train_merge =  merge_features(X_train_abs , X_train_titkw)\n",
    "X_test_merge = merge_features(X_test_abs, X_test_titkw)\n",
    "\n",
    "#scale data\n",
    "X_train = data_scaler(X_train_merge, train=True)\n",
    "X_test = data_scaler(X_test_merge,train=False)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(type(X_train))\n",
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "              min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=0.8, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "strategy = 'classifier_chains'\n",
    "model =  train_model(X_train, y_train, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "train_proba = model.predict_proba(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "test_proba = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_select(X_train,y_train,X_test,y_test):\n",
    "    xgb = XGBClassifier(n_estimators=500)\n",
    "    rf = RandomForestClassifier(max_depth=3,n_estimators=5000,n_jobs=-1)\n",
    "    svm = SVC()\n",
    "    gbdt = GradientBoostingClassifier()\n",
    "    lr = linear_model.LogisticRegression()\n",
    "    models = {'Xgboost':xgb, 'RandomForests': rf, 'SVM':svm,'GBDT':gbdt,\"LogReg\":lr}\n",
    "    for name, model in models.items():\n",
    "        cc = ClassifierChain(model)\n",
    "        print(cc)\n",
    "        cc.fit(X_train,y_train)\n",
    "        predictions = cc.predict(X_test)\n",
    "        print(\"classification report of %s: \" % name )\n",
    "        print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierChain(classifier=XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                         colsample_bylevel=1,\n",
      "                                         colsample_bynode=1, colsample_bytree=1,\n",
      "                                         gamma=0, learning_rate=0.1,\n",
      "                                         max_delta_step=0, max_depth=3,\n",
      "                                         min_child_weight=1, missing=None,\n",
      "                                         n_estimators=500, n_jobs=1,\n",
      "                                         nthread=None,\n",
      "                                         objective='binary:logistic',\n",
      "                                         random_state=0, reg_alpha=0,\n",
      "                                         reg_lambda=1, scale_pos_weight=1,\n",
      "                                         seed=None, silent=None, subsample=1,\n",
      "                                         verbosity=1),\n",
      "                order=None, require_dense=[True, True])\n",
      "classification report of Xgboost: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89         5\n",
      "           1       0.75      0.50      0.60        12\n",
      "           2       0.80      0.89      0.84        27\n",
      "\n",
      "   micro avg       0.81      0.77      0.79        44\n",
      "   macro avg       0.85      0.73      0.78        44\n",
      "weighted avg       0.81      0.77      0.78        44\n",
      " samples avg       0.82      0.79      0.80        44\n",
      "\n",
      "ClassifierChain(classifier=RandomForestClassifier(bootstrap=True,\n",
      "                                                  class_weight=None,\n",
      "                                                  criterion='gini', max_depth=3,\n",
      "                                                  max_features='auto',\n",
      "                                                  max_leaf_nodes=None,\n",
      "                                                  min_impurity_decrease=0.0,\n",
      "                                                  min_impurity_split=None,\n",
      "                                                  min_samples_leaf=1,\n",
      "                                                  min_samples_split=2,\n",
      "                                                  min_weight_fraction_leaf=0.0,\n",
      "                                                  n_estimators=5000, n_jobs=-1,\n",
      "                                                  oob_score=False,\n",
      "                                                  random_state=None, verbose=0,\n",
      "                                                  warm_start=False),\n",
      "                order=None, require_dense=[True, True])\n",
      "classification report of RandomForests: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        12\n",
      "           2       0.66      1.00      0.79        27\n",
      "\n",
      "   micro avg       0.66      0.61      0.64        44\n",
      "   macro avg       0.22      0.33      0.26        44\n",
      "weighted avg       0.40      0.61      0.49        44\n",
      " samples avg       0.66      0.62      0.63        44\n",
      "\n",
      "ClassifierChain(classifier=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                               coef0=0.0, decision_function_shape='ovr',\n",
      "                               degree=3, gamma='auto_deprecated', kernel='rbf',\n",
      "                               max_iter=-1, probability=False,\n",
      "                               random_state=None, shrinking=True, tol=0.001,\n",
      "                               verbose=False),\n",
      "                order=None, require_dense=[True, True])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report of SVM: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        12\n",
      "           2       0.69      1.00      0.82        27\n",
      "\n",
      "   micro avg       0.69      0.61      0.65        44\n",
      "   macro avg       0.23      0.33      0.27        44\n",
      "weighted avg       0.42      0.61      0.50        44\n",
      " samples avg       0.66      0.62      0.63        44\n",
      "\n",
      "ClassifierChain(classifier=GradientBoostingClassifier(criterion='friedman_mse',\n",
      "                                                      init=None,\n",
      "                                                      learning_rate=0.1,\n",
      "                                                      loss='deviance',\n",
      "                                                      max_depth=3,\n",
      "                                                      max_features=None,\n",
      "                                                      max_leaf_nodes=None,\n",
      "                                                      min_impurity_decrease=0.0,\n",
      "                                                      min_impurity_split=None,\n",
      "                                                      min_samples_leaf=1,\n",
      "                                                      min_samples_split=2,\n",
      "                                                      min_weight_fraction_leaf=0.0,\n",
      "                                                      n_estimators=100,\n",
      "                                                      n_iter_no_change=None,\n",
      "                                                      presort='auto',\n",
      "                                                      random_state=None,\n",
      "                                                      subsample=1.0, tol=0.0001,\n",
      "                                                      validation_fraction=0.1,\n",
      "                                                      verbose=0,\n",
      "                                                      warm_start=False),\n",
      "                order=None, require_dense=[True, True])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report of GBDT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89         5\n",
      "           1       0.67      0.33      0.44        12\n",
      "           2       0.77      0.89      0.83        27\n",
      "\n",
      "   micro avg       0.78      0.73      0.75        44\n",
      "   macro avg       0.81      0.67      0.72        44\n",
      "weighted avg       0.77      0.73      0.73        44\n",
      " samples avg       0.78      0.74      0.76        44\n",
      "\n",
      "ClassifierChain(classifier=LogisticRegression(C=1.0, class_weight=None,\n",
      "                                              dual=False, fit_intercept=True,\n",
      "                                              intercept_scaling=1,\n",
      "                                              l1_ratio=None, max_iter=100,\n",
      "                                              multi_class='warn', n_jobs=None,\n",
      "                                              penalty='l2', random_state=None,\n",
      "                                              solver='warn', tol=0.0001,\n",
      "                                              verbose=0, warm_start=False),\n",
      "                order=None, require_dense=[True, True])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report of LogReg: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.71         5\n",
      "           1       0.50      0.42      0.45        12\n",
      "           2       0.77      0.74      0.75        27\n",
      "\n",
      "   micro avg       0.67      0.68      0.67        44\n",
      "   macro avg       0.61      0.72      0.64        44\n",
      "weighted avg       0.67      0.68      0.67        44\n",
      " samples avg       0.70      0.70      0.68        44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model_select(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_seach(data, target, model,params):\n",
    "    model_tosearch = ClassifierChain(model)\n",
    "    model_tunning = GridSearchCV(model_tosearch, cv=5, param_grid=params,\n",
    "                                 scoring='f1_weighted',verbose=5, n_jobs=-1)\n",
    "    model_tunning.fit(data,target)\n",
    "    print(model_tunning.best_score_)\n",
    "    print(model_tunning.best_params_)\n",
    "    print(model_tunning.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 24.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 28.7min finished\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883003959738369\n",
      "{'classifier__colsample_bytree': 0.8, 'classifier__max_depth': 7, 'classifier__subsample': 0.8}\n",
      "ClassifierChain(classifier=XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                         colsample_bylevel=1,\n",
      "                                         colsample_bynode=1,\n",
      "                                         colsample_bytree=0.8, gamma=0,\n",
      "                                         learning_rate=0.1, max_delta_step=0,\n",
      "                                         max_depth=7, min_child_weight=1,\n",
      "                                         missing=None, n_estimators=150,\n",
      "                                         n_jobs=1, nthread=None,\n",
      "                                         objective='binary:logistic',\n",
      "                                         random_state=0, reg_alpha=0,\n",
      "                                         reg_lambda=1, scale_pos_weight=1,\n",
      "                                         seed=None, silent=None, subsample=0.8,\n",
      "                                         verbosity=1),\n",
      "                order=None, require_dense=[True, True])\n"
     ]
    }
   ],
   "source": [
    "params = {'classifier__max_depth':range(3,10,2),\n",
    "              'classifier__subsample':[0.6,0.7,0.8,0.9,1],\n",
    "              'classifier__colsample_bytree':[0.6,0.7,0.8,0.9,1]}\n",
    "model = XGBClassifier(learning_rate =0.1, n_estimators=150)\n",
    "params_seach(X_train,y_train,model,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:   49.1s remaining:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0      15.614905      0.356668         0.037699        0.007035   \n",
      "1      14.792997      0.080744         0.037700        0.004610   \n",
      "2      13.759011      0.817348         0.039096        0.008472   \n",
      "\n",
      "  param_classifier__learning_rate) param_classifier__n_estimators  \\\n",
      "0                            0.001                            150   \n",
      "1                             0.01                            150   \n",
      "2                              0.1                            150   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'classifier__learning_rate)': 0.001, 'classif...           0.738273   \n",
      "1  {'classifier__learning_rate)': 0.01, 'classifi...           0.738273   \n",
      "2  {'classifier__learning_rate)': 0.1, 'classifie...           0.738273   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "0           0.730431           0.525307           0.766314           0.680491   \n",
      "1           0.730431           0.525307           0.766314           0.680491   \n",
      "2           0.730431           0.525307           0.766314           0.680491   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  \n",
      "0           0.6883        0.085929                1  \n",
      "1           0.6883        0.085929                1  \n",
      "2           0.6883        0.085929                1  \n",
      "0.6883003959738369\n",
      "{'classifier__learning_rate)': 0.001, 'classifier__n_estimators': 150}\n",
      "ClassifierChain(classifier=XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                         colsample_bylevel=1,\n",
      "                                         colsample_bynode=1,\n",
      "                                         colsample_bytree=0.8, gamma=0,\n",
      "                                         learning_rate=0.1,\n",
      "                                         learning_rate)=0.001, max_delta_step=0,\n",
      "                                         max_depth=7, min_child_weight=1,\n",
      "                                         missing=None, n_estimators=150,\n",
      "                                         n_jobs=1, nthread=None,\n",
      "                                         objective='binary:logistic',\n",
      "                                         random_state=0, reg_alpha=0,\n",
      "                                         reg_lambda=1, scale_pos_weight=1,\n",
      "                                         seed=None, silent=None, subsample=0.8,\n",
      "                                         verbosity=1),\n",
      "                order=None, require_dense=[True, True])\n"
     ]
    }
   ],
   "source": [
    "params = {'classifier__learning_rate)':[0.001,0.01,0.1],\n",
    "              'classifier__n_estimators':[150]}\n",
    "model = XGBClassifier(max_depth=7, subsample=0.8, colsample_bytree=0.8)\n",
    "params_seach(X_train,y_train,model,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=7, subsample=0.8, colsample_bytree=0.8, learning_rate=0.001, n_estimators=150)\n",
    "cc = ClassifierChain(xgb)\n",
    "cc.fit(X_train,y_train)\n",
    "preds = cc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       0.67      0.17      0.27        12\n",
      "           2       0.72      0.96      0.83        27\n",
      "\n",
      "   micro avg       0.73      0.68      0.71        44\n",
      "   macro avg       0.80      0.51      0.55        44\n",
      "weighted avg       0.74      0.68      0.64        44\n",
      " samples avg       0.73      0.70      0.71        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.580700623874039\n",
      "{'classifier__max_depth': 9, 'classifier__max_features': 'auto', 'classifier__n_estimators': 100}\n",
      "ClassifierChain(classifier=RandomForestClassifier(bootstrap=True,\n",
      "                                                  class_weight=None,\n",
      "                                                  criterion='gini', max_depth=9,\n",
      "                                                  max_features='auto',\n",
      "                                                  max_leaf_nodes=None,\n",
      "                                                  min_impurity_decrease=0.0,\n",
      "                                                  min_impurity_split=None,\n",
      "                                                  min_samples_leaf=1,\n",
      "                                                  min_samples_split=2,\n",
      "                                                  min_weight_fraction_leaf=0.0,\n",
      "                                                  n_estimators=100, n_jobs=-1,\n",
      "                                                  oob_score=False,\n",
      "                                                  random_state=None, verbose=0,\n",
      "                                                  warm_start=False),\n",
      "                order=None, require_dense=[True, True])\n"
     ]
    }
   ],
   "source": [
    "params = {'classifier__max_depth':range(3,10,2),\n",
    "          'classifier__n_estimators':[100,200,300,400],\n",
    "          'classifier__max_features':['auto','log2','sqrt']\n",
    "         }\n",
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "params_seach(X_train,y_train,model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
